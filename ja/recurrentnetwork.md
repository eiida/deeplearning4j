---
title: "Tutorial: Recurrent Networks and LSTMs in Java"
layout: default
---

# チュートリアル: 再帰型ネットワークと長・短期記憶

再帰型ニューラルネットワークは、人工のニューラルネットワークの強力なアルゴリズム一式です。音声、時系列（センサー）データ、書かれた自然言語などの連続データを処理するのに役立ちます。再帰型ネットワークのあるバージョンを使用した例として、ビデオゲームのプレイに自律エージェントを参加させたDeepMind社があります。

再帰型ネットワークは、フィードバックループを使うという点で順伝播型とは異なります。再帰型ネットワークでは、ステップn-1の出力をステップnに提供して出力に反映する、という作業をすべてのステップについて行います。例えば、ある単語の文字が1文字づつネットワークに提供され、次の文字を推測するよう指示されるような場合、その単語のある文字が再帰型ネットワークにとっては次の文字を推測する助けとなるのです。 

一方、順伝播型ネットワークの場合はこれとは異なり、MNISTの手書きの各数字の分類を学習するのには個々のサンプルの画素を使い、その前のサンプルを参考にして予測を調節することはありません。順伝播型ネットワークは入力を一度に1つだけ受け入れ、1つだけ出力します。再帰型ネットワークの場合、このような1:1の関係に制約されていません。

画像など、データの形式によっては非連続的に思えるものもありますが、再帰型ネットワークに入力させると連続的データであると見なされます。手書きの単語の画像を例に挙げてみましょう。手書きの文字を再帰型ネットワークが処理するときに各々の曲線画像をある1文字に変換したり、最初の文字からある単語の終わりを推測したりするように、ネットワークは画像をどの部分であっても連続的なものとして扱います。大きなイメージの上を移動するニューラルネットワークはそれぞれの領域から隣接する領域の推測を行うのです。  

## ニューラルネットワークのモデルと記憶

ある意味で、再帰型も順伝播型もデータをモデル化することにより世界の何かについてを「記憶しています」。しかし、その記憶の仕方が随分異なります。トレ―ニング後に、順伝播型ネットワークは扱ったデータの静的モデルを作成します。そしてそのモデルが新しく入力されたサンプルを正確に分類したり、クラスタリングするのです。 

一方、再帰型ネットは、扱うサンプルのコンテキストに依存した分類を正確に行うよう、時間とともに力動的に変化するモデルを作成します。 

正確に言うと、再帰型モデルには前の分類を連続的に決定した隠れ状態があります。後続する各ステップにおいて、この隠れ状態は新しいステップの入力データと組み合わせられ、新しい隠れ状態と分類を作成します。各隠れ状態は、後に続く隠れ状態を改善させるために再利用されます。 

人間の記憶もコンテキストを意識したものであり、新しいデータを正しく解釈するために過去の状態に対する認識を再利用します。例えば、ある二人の人を例に挙げてみましょう。一人は自分がジャックの家の近くにいることを認識しています。もう一人は自分が飛行機に入ったことを認識しています。この二人は、「やあ、ジャック！」という音声を全く異なる方法で解釈します。これは人間が短期記憶や過去の感覚の影響を受けた隠れ状態を持っているからです。 

このように、異なる短期記憶がそれぞれ異なる時に呼び起されます。これは現在の入力に正しい意味を割り当てるためです。これらの記憶には最近作られたものから、必要となる時よりずいぶん前の何ステップも前に作られたものまであります。記憶と時間的に離れた入力を効果的に関連付ける再帰型ネットワークは [長・短期記憶（LSTM）](https://deeplearning4j.org/ja/lstm)と呼ばれています。

## ユースケース

再帰型ネットワークには予測能力があり、データの構造を時間をかけて力動的に理解し、連続的に次の要素を予測します。これらの要素は、単語であれば次の文字であり、文であれば次の単語（自然言語の生成）であり、センサー、経済学の表、株価の値動きのデータであれば次の数字などとなります。

連続データにはビデオも含まれ、再帰型ネットワークは、[リアルタイムでの物体と身振りの追跡](http://arxiv.org/abs/1503.08909)に使用されています。

再帰型ネットワークは、他のニューラルネットワークと同様、クラスタリングや異常検出に役立ちます。つまり、ベクトル空間のサンプルをグループに分けてグループ間の距離を測定することにより類似性と相違を認識します。正常な行動のモデル化、異常な行動へのフラグ付けは、装着するものから作成される医療データ、サームスタットなどの高性能なものから作成された家庭データ、株や指標から作成される市場データ、口座取引から作成された個人的財政データに適用できます。

## コードのサンプル

Deeplearning4jのマルチネットワーク設定により、名前を付けるだけでAPIに層を作成することができます。この場合、長・短期記憶を作成します。 

<script src="http://gist-it.appspot.com/https://github.com/deeplearning4j/dl4j-examples/blob/master/src/main/java/org/deeplearning4j/examples/rnn/GravesLSTMCharModellingExample.java?slice=48:218"></script>

### 長・短期記憶をDL4Jに実装

* [Graves LSTM](https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/recurrent/character/GravesLSTMCharModellingExample.java) （センサーデータと時系列に役立ちます。）


## 短期記憶用データの読み込み

Deeplearning4jでは、通常、長・短期記憶が扱う行列は最初の行が**x_i**であり、ニューラルネットワークが予測するのはその後に続く行の**x_s**です。これは生成モデルで、ラベルがありません。行列の行数はいくつでもいいのですが、すべての行が同じ長さでなければなりません。 

Graves長・短期記憶は多層ネットワークで使用するためのものです。 

入力データの例: 

* 各行の長さが784エレメントであるMNISTの手書きの数字 
* 各行の長さが1エレメントであるアルファベットの文字列  

### リソース

* [Recurrent Neural Networks](http://people.idsia.ch/~juergen/rnn.html)（再帰型ニューラルネットワーク）; Juergen Schmidhuber著
* [Modeling Sequences With RNNs and LSTMs](https://class.coursera.org/neuralnets-2012-001/lecture/77)（再帰型ニューラルネットワークと長・短期記憶によるシーケンスモデリング）; Geoff Hinton著
* [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)（想像以上に効果のある再帰型ニューラルネットワーク）; Andrej Karpathy著

### 再帰型ニューラルネットワークとは

再帰型ニューラルネットワークは、「並列計算を連続的に行うことが可能で、従来のコンピューターが計算できたこと大体すべてが計算できます。しかし、従来のコンピューターとは異なり、再帰型ニューラルネットワークは、人間の脳のように相互接続したニューロンで構成された大型のフィードバック機能を持ったネットワークであり、長く続く感覚的入力の流れを有益な動的出力の連続へと翻訳することを学習できます。脳が優れた手本になっており、現在のマシンが解決できない問題の多くを解決できます。」- Juergen Schmidhuber

### 再帰型ネットワークの研究の貢献者

*再帰型ネットワークの研究の多くはJuergen Schmidhuberとその学生達によって引率されているものです。その中でもSepp Hochreiterは非常に層の深いネットワークに発生する消失勾配の問題を発見し、現在DeepMind所属のAlex Gravesと長・短期記憶再帰型ネットワークを発明しました。その他の特筆に値する研究者には、長・短期記憶の忘却ゲートを発明したFelix Gersや様々な種類の長・短期記憶のトポロジを個々の問題に応じて自動進化させる方法を編み出したJustin Bayerがいます。
